{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\">Uncertainty</div>\n",
    "\n",
    "<div style=\"text-align: center\"> <sub>ENCN404 - Modern Modelling Practices in Civil Engineering</sub></div>\n",
    "\n",
    "$\\,$\n",
    "\n",
    "<div style=\"text-align: center\"> University of Canterbury </div>\n",
    "\n",
    "$\\,$\n",
    "\n",
    "<img src=\"img/unc_front.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and environment: this cell must be executed before any other in the notebook\n",
    "from uncertainty404 import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Uncertainty in Mathematical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using mathematical models to describe the real-world, we must contend with a staggering amount of **ignorance**<sup>1</sup>. Indeed, mathematical modelling as an activity is motivated by our own ignorance. Perhaps we are interested in the unknown value of some system parameter? Or maybe we harbour a general concern about the unpredictable future? Or, turning it around, if we already **know** the answer, what is the point of modelling?\n",
    "\n",
    "**Minimizing** the effects of error and uncertainty, and articulating **quantitatively** that which remains, are nontrivial undertakings. Too often, these steps are neglected: a model prediction is made that, in the absence of any disclosure of uncertainty, is **implied to be perfect**. Vague caveats might sometimes be offered about the prediction's quality in the face of some model simplification or assumed parameter value. However, the greatest confidence is attained when model predictions are attached **probabilistic bounds**, e.g., \"*by 2024, the largest earthquake triggered by gas extraction operations has a 90% likelihood of lying in the interval [3.8, 4.6], with the most likely outcome being a M 4.2*\".\n",
    "\n",
    "Error and uncertainty enter into the modelling process **at all stages**, for instance: \n",
    "- When a model is first developed and we are required to make **simplifying assumptions** (structural error).\n",
    "- When **imperfect or incomplete measurements** (observation error) are used to calibrate the model.\n",
    "- When the model is used to **make a prediction** of the future (prediction uncertainty), even though its parameters are **not completely constrained** (parameter uncertainty).\n",
    "\n",
    "Because uncertainty can never be eliminated entirely, it is important that rigorous methods are developed to handle it. For this module, you will be assessed on your ability to, for a specific scenario, **identify** where significant uncertainty could arise and, where possible, **quantify** its impact on modelling outcomes.\n",
    "\n",
    "<sup>1</sup> <sub> Note, this is not the type of ignorance that carries negative connotations, i.e., obstinate closed-mindedness, an indifference to (or even a conscious intent to ignore) fact or reason. Rather, we are referring to the gaps in our knowledge: always present, but recognised, and with a desire to minimise.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1\tObservational error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a quantity, $y_i$, is measured twice in succession, $\\tilde{y}_i^{(1)}$ and $\\tilde{y}_i^{(2)}$, it is unlikely that the same value will be obtained. Assuming that the true<sup>2</sup> underlying value, $y_{i,0}$, is not changing, differences between repeated measurements, $\\tilde{y}_i^{(j)}$ are generally attributed to **random error**, $\\epsilon$, that reflects instrument limitations or uncontrollable variables. In the absence of systematic error, the measurement process is expressed algebraically\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{y}_i^{(j)}=y_{i,0}+\\epsilon,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\epsilon$ is a normally distributed random variable with zero mean and variance, $\\sigma_i^2$, i.e., $\\epsilon\\sim\\mathcal{N}(0,\\sigma_i^2)$. While any particular measurement might be contaminated with significant random error, as we take the mean of **repeated measurements**, the effects of measurement error will be averaged out.\n",
    "\n",
    "We could replace the true value of $y_i$ above with the model, $f(x_i;\\boldsymbol{\\theta})$, to obtain a model for the observation process\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{y}_i (\\boldsymbol{\\theta})=f(x_i;\\boldsymbol{\\theta})+\\mathcal{N}(0,\\sigma_i^2 ),\n",
    "\\end{equation}\n",
    "\n",
    "which is a normal distribution centred on the model prediction, $f(x_i;\\boldsymbol{\\theta})$, i.e., $P(\\tilde{y}_i│\\boldsymbol{\\theta})\\sim\\mathcal{N}(f(x_i;\\boldsymbol{\\theta}),\\sigma_i^2)$. \n",
    "\n",
    "The notation $P(\\tilde{y}_i│\\boldsymbol{\\theta})$ should be read (slowly, several times over in your head) \"*the probability of obtaining an observation, $\\tilde{y}_i$, given a model of the process, $f(\\cdot)$, which has the parameters, $\\boldsymbol{\\theta}$*\" and it expresses the idea that \"*the (underlying, true world) parameters **determine** the observations*\". \n",
    "\n",
    "**Observations** are the critical ingredient in model calibration. They serve as a sort of target that the model should be able to, at least approximately, replicate. What should we do then when the target is fuzzy or partially obscured by observational error? Observations provide **information** about parameters, and the purpose of calibration is to extract that information. Thus a consequence of observational error is to place **limits** on what can be learned about the parameters.\n",
    "\n",
    "<sup>2</sup> <sub> We shall use the subscript 0 to indicate a quantity is the \"*true*\", i.e., real world, value.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 A Linear Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We shall return to the example below throughout this notebook.***\n",
    "\n",
    "Consider some physical process that happens to be well-described by a linear model. Linear models have two parameters: a gradient, $m$, and $y$-intercept, $c$, i.e., $y_i=f(x_i;m,c)=mx_i+c$. For the purposes of demonstration, we shall cast ourselves in the role of **Omniscient Supreme Being**, in which case we are able to “know” the true parameter values: $m=2$ and $c=3$, or $\\boldsymbol{\\theta}_0=[2.0,3.0]$. However, the less-enlightened humans on the ground<sup>3</sup> don’t have access to this information, and must instead satisfy themselves by making measurements and developing a model. \n",
    "\n",
    "Being government employees, their instruments are not wildly sensitive ($\\sigma_i^2=0.1$) and there is only budget to make ten observations, $\\tilde{y}_i$. Remember: as almighty overlords, we can see both the true model (blue line) and the observations (open circles), but the poor humans can only see the latter.\n",
    "\n",
    "<sup>3</sup> <sub> Simple-minded plebs.</sub>\n",
    "\n",
    "***Execute the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the slider N_obs to show more or less observations.\n",
    "\n",
    "# What do the plotted normal distributions represent? \n",
    "\n",
    "# Click the \"True Process\" checkbox. What does the centre of each normal distribution correspond to?\n",
    "\n",
    "# Click the \"Best Model\" checkbox. Do the Best and True models agree? Why or why not?\n",
    "\n",
    "# Click \"ROLL THE DICE\" and consider (i) the observations, (ii) the Best Model, and (iii) the True Process.\n",
    "# Which ones have changed and which ones have stayed the same?\n",
    "\n",
    "# Move the variance slider.\n",
    "# How does agreement between Best and True models change as the variance decreases?\n",
    "\n",
    "# Move the N_obs slider.\n",
    "# How does agreement between Best and True models change as the more observations are taken?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 3.1.2 A Geothermal Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***We shall return to the example below throughout this notebook.***\n",
    "\n",
    "Hot water and steam have been extracted from the Wairakei geothermal system for the last 60 years, used to produce clean, renewable electricity. As water was extracted, pressure in the reservoir started to drop, eventually stabilizing.\n",
    "\n",
    "***Execute the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wairakei_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# From 1965, the production rate declines but is still quite high. \n",
    "\n",
    "# Suggest a reason why the pressure eventually stops declining.\n",
    "# (Think about supplying heat to the centre of a metal plate - the temperature\n",
    "# can't keep going up forever...)\n",
    "\n",
    "# Does one of these curves 'cause' the other?\n",
    "\n",
    "# If we were to build a model of the reservoir, would 'production' be an input \n",
    "# or an output? What about 'pressure'?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***The Lumped Parameter Model***\n",
    "\n",
    "A geothermal reservoir can be conceptualised as a **cylindrical** volume. (***Execute the cell below and slide through.***)\n",
    "\n",
    "1. Initially, the reservoir pressure is in equilibrium with the surrounding rock.\n",
    "2. Water can **exit** the cylinder (usually from wells drilled into the middle). When it does, the pressure goes **down**.\n",
    "3. When the pressure drops, more water will try to **enter** the cylinder (usually at the base or the sides). When it does this, the pressure goes **up**.\n",
    "4. The pressure drop due to extracting water is not immediate. Rather, there is a delay over time called \"slow drainage\". This introduces a dependence on the **rate of change of extraction rate**.\n",
    "\n",
    "A **lumped parameter model** (LPM) describes the average pressure evolution of the reservoir as a whole. As an ODE, a particular type of LPM can be expressed (from [Fradkin et al. [1981]](http://onlinelibrary.wiley.com/doi/10.1029/WR017i004p00929/full)):\n",
    "\n",
    "$$ \\frac{dP}{dt} = - a q - b P - c \\frac{dq}{dt}$$\n",
    "\n",
    "were $P$ is the pressure change from the initial value (not the absolute pressure), $q$ is the extraction rate from the reservoir, and $a$, $b$ and $c$ are unknown parameters that depend on the reservoir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lpm_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Coefficient 'a' must always be positive (for positive production, q). Why?\n",
    "\n",
    "# Coefficient 'b' must always be positive. Why?\n",
    "\n",
    "# If the production rate is constant, this ODE has only two parameters. Why?\n",
    "\n",
    "# Solve the ODE analytically for constant production, q0. \n",
    "# How does the solution behave?\n",
    "# What does the characteristic time depend on?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The lumped parameter model has been implemented for you in the cell below. Because $q$ does not have a nice closed form expression, we use an improved Euler step to solve the ODE.\n",
    "\n",
    "***Execute the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load flow rate data and compute derivative\n",
    "tq,q = np.genfromtxt('wk_production_history.csv', delimiter = ',')[:28,:].T\n",
    "tp,p = np.genfromtxt('wk_pressure_history.csv', delimiter = ',')[:28,:].T\n",
    "\n",
    "dqdt = 0.*q                 # allocate derivative vector\n",
    "dqdt[1:-1] = (q[2:]-q[:-2])/(tq[2:]-tq[:-2])    # central differences\n",
    "dqdt[0] = (q[1]-q[0])/(tq[1]-tq[0])             # forward difference\n",
    "dqdt[-1] = (q[-1]-q[-2])/(tq[-1]-tq[-2])        # backward difference\n",
    "\n",
    "# define derivative function\n",
    "def lpm(pi,t,a,b,c):                 # order of variables important\n",
    "    qi = np.interp(t,tq,q)           # interpolate (piecewise linear) flow rate\n",
    "    dqdti = np.interp(t,tq,dqdt)     # interpolate derivative\n",
    "    return -a*qi - b*pi - c*dqdti    # compute derivative\n",
    "\n",
    "# implement an imporved Euler step to solve the ODE\n",
    "def solve_lpm(t,a,b,c):\n",
    "    pm = [p[0],]                            # initial value\n",
    "    for t0,t1 in zip(tp[:-1],tp[1:]):           # solve at pressure steps\n",
    "        dpdt1 = lpm(pm[-1]-p[0], t0, a, b, c)   # predictor gradient\n",
    "        pp = pm[-1] + dpdt1*(t1-t0)             # predictor step\n",
    "        dpdt2 = lpm(pp-p[0], t1, a, b, c)       # corrector gradient\n",
    "        pm.append(pm[-1] + 0.5*(t1-t0)*(dpdt2+dpdt1))  # corrector step\n",
    "    return np.interp(t, tp, pm)             # interp onto requested times\n",
    "    \n",
    "# use CURVE_FIT to find \"best\" model\n",
    "from scipy.optimize import curve_fit\n",
    "pars = curve_fit(solve_lpm, tp, p, [1,1,1])[0]\n",
    "\n",
    "# plot the best solution\n",
    "pm = solve_lpm(tp,*pars)\n",
    "f,ax = plt.subplots(1,1,figsize=(12,6))\n",
    "ax.plot(tp, p, 'ro', label = 'observations')\n",
    "ax.plot(tp, pm, 'k-', label='model')\n",
    "ax.set_ylabel(\"pressure [bar]\",size=14); ax.set_xlabel(\"time\",size=14)\n",
    "ax.legend(prop={'size':14})\n",
    "ax.set_ylim([25,60])\n",
    "ax.set_title('a={:2.1e},   b={:2.1e},   c={:2.1e}'.format(*pars),size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# In this model, P is given in bar, time in years, q in kg/s, and dqdt in kg/s/year. \n",
    "# What are the dimensions of parameters 'a', 'b' and 'c'? What are the equivalent SI units?\n",
    "\n",
    "# Convert the equation above so it operates in SI units. Does curve_fit return values for\n",
    "# 'a', 'b' and 'c' you calculated above?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In fact, there is some uncertainty in the measured pressure data. Reservoir engineers estimate the error variance could be as large as 2 bar. Thus, it seems reasonable that there are a range of models - a range for 'a', 'b' and 'c' - that could fit the data.\n",
    "\n",
    "***Execute the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lpm_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Move the slider bars to determine acceptable ranges for the parameters 'a', 'b' and 'c'.\n",
    "\n",
    "# Set the slider bars to their original positions.\n",
    "\n",
    "# Change 'a' to 0.00264. Is this an acceptable model?\n",
    "\n",
    "# Leave 'a' as 0.00264 and change 'b' to 0.132. Is this an acceptable model?\n",
    "\n",
    "# Is a=0.00264 a plausible parameter value? Justify your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2\tParameter uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a prediction with a model, a conscious choice must first be made about the values of **parameters** used as model inputs. However, parameters values are never known with **absolute certainty**. Instead, they are better characterised by a distribution<sup>4</sup>, which expresses our **belief about the true value** for that parameter. The two most common distributions are the uniform and normal distributions. \n",
    "\n",
    "Distributions express our ignorance. The **uniform** distribution bounds the value of a parameter, $\\boldsymbol{\\theta}_i$, between minimum and maximum values, $[\\boldsymbol{\\theta}_{min},\\boldsymbol{\\theta}_{max}]$ and expresses the notion \"*the true parameter value is somewhere in this range, but I have no idea where*\". The **normal** distribution goes further, expressing what we think is the most likely value of the parameter (the mean) but also attaching a degree of uncertainty (the variance). Rarely, though, will the true parameter value turn out to be exactly the mean<sup>5</sup>. \n",
    "\n",
    "<sup>4</sup> <sub> For some parameters, the distribution might be quite narrow, e.g., the value of gravity. But even gravitational constants are only known to a finite level of precision.</sub>\n",
    "\n",
    "<sup>5</sup> <sub> A pregnant woman’s \"due date\" is the **most-likely** date of birth given the estimated date of conception and a distribution for the gestation time of a human foetus. That being said, only 4% of babies are actually born on their due date. So \"most-likely\" is certainly not the same as \"likely\".</sub>\n",
    "\n",
    "***Execute the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the area under the uniform distribution (left)?\n",
    "\n",
    "# How does the height of the uniform distribution change as you slide the limits left and right?\n",
    "\n",
    "# What is the area under the normal distribution (right)?\n",
    "\n",
    "# Does a uniform distribution categorically rule out any parameter values? What about the normal distribution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a parameter distribution is presented by the modeller based solely on consultation of the relevant literature or by drawing on their own expert knowledge, then it is called a **prior**. In Bayesian terms, the prior is a quantitative statement of belief (or ignorance) about a parameter’s value. When a set of observations, $\\tilde{y}_i$, is used to provide further constraints on the parameter’s value (say, through model calibration), then we obtain a new distribution, called the **posterior**. Again, this is a quantitative statement of belief that, now, incorporates the relevant observations alongside our prior notions.\n",
    "\n",
    "For the case of a uniform prior (\"*I have no idea what the value of this parameter is...*\"), a reformulation of the weighted sum-of-squares objective function, $S(\\boldsymbol{\\theta})$, provides a simple way to determine the posterior parameter distribution. Defining $S(\\boldsymbol{\\theta})$ as<sup>6</sup>:\n",
    "\n",
    "\\begin{equation}\n",
    "\tS(\\boldsymbol{\\theta})=\\sum\\limits_{i=1}^N\\frac{1}{\\sigma_i^2}\\left(\\tilde{y}_i-f(x_i;\\boldsymbol{\\theta})\\right)^2,\n",
    "\\end{equation}\n",
    "\n",
    "then the posterior distribution over multi-dimensional parameter space<sup>7</sup> is \n",
    "\n",
    "\\begin{equation}\n",
    "\tP(\\boldsymbol{\\theta}|\\tilde{y}_i)=A e^{-S(\\boldsymbol{\\theta})/2},\n",
    "\\end{equation}\t\n",
    "\n",
    "where $A$ is a normalising constant for the probability distribution. A proper understanding of what $P(\\boldsymbol{\\theta}|\\tilde{y}_i)$ actually represents requires a little quiet introspection on the following points:\n",
    "1. **Information** about the true parameters, $\\boldsymbol{\\theta}_0$, is obtained by making **measurements**, $\\tilde{y}_i$.\n",
    "2. All measurements, even the most meticulous ones, contain some amount of **error**, $\\epsilon$.\n",
    "3. Therefore, the true parameters, $\\boldsymbol{\\theta}_0$, can **never be determined** exactly.\n",
    "4. However, we can find parameters, $\\hat{\\boldsymbol{\\theta}}_0$, that give a model **most consistent** with the data (found by minimising $S(\\boldsymbol{\\theta})$, i.e., calibration).\n",
    "5. We concede that $\\hat{\\boldsymbol{\\theta}}_0$ is not equal to $\\boldsymbol{\\theta}_0$, although it is **hopefully close**.\n",
    "6. Therefore, it is incumbent upon us to consider the multitude of other parameter sets, $\\boldsymbol{\\theta}$, that are near to $\\hat{\\boldsymbol{\\theta}}_0$, and thus are also **possible candidates** for $\\boldsymbol{\\theta}_0$.\n",
    "\n",
    "The equation for $\\tilde{y}_i (\\boldsymbol{\\theta})$ in Section 3.1 expresses a forward model for the measurement process: parameters ($\\boldsymbol{\\theta}$) determine observations ($\\tilde{y}_i$). The posterior distribution $P(\\boldsymbol{\\theta}|\\tilde{y}_i)$ expresses exactly the **opposite**, that observations determine<sup>8</sup> the parameters, or \"*what is the probability that the true parameter values are $\\boldsymbol{\\theta}$, given that I have made measurements, $\\tilde{y}_i$*\"<sup>9</sup>. Again, because $P(\\boldsymbol{\\theta}│\\tilde{y}_i)$ is a distribution, it expresses both our knowledge (\"*These values are likely...*\") and our ignorance (\"*...but we can't choose just one*\").\n",
    "\n",
    "<sup>6</sup> <sub> Note, compared to the definition in the Calibration notebook, the weights on each term are the reciprocal of the variance for the associated measurement error. Fun fact to annoy your friends: when a set of measurements have different error, $\\sigma_i^2$, they are said to be **heteroskedastic**.</sub>\n",
    "\n",
    "<sup>7</sup> <sub> For a derivation, see Appendix A. This is not examinable.</sub>\n",
    "\n",
    "<sup>8</sup> <sub> We don’t mean \"determine\" in the **deterministic** sense, i.e., to cause the parameters to take on particular values. Rather, we refer to the **inference** sense, i.e., to provide the information necessary to constrain the parameter values.</sub>\n",
    "\n",
    "<sup>9</sup> <sub> You have probably heard the old trope \"*we cannot prove a hypothesis, only falsify it*\". The posterior distribution has a nice interpretation in this context: each parameter set represents the hypothesis \"*this is the true parameter set, $\\boldsymbol{\\theta}_0$*\". By computing the posterior, we falsify (probabilistically) some of those hypotheses.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1\tComputing posterior parameter distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the definition above, to compute the posterior we first need to calculate the **sum-of-squares objective function**, $S(\\boldsymbol{\\theta})$, a surface over multi-dimensional parameter space. For our purposes, it is sufficient to approximate this surface by computing $S(\\boldsymbol{\\theta}_k)$, where $\\boldsymbol{\\theta}_k$ are the points of a **discretised grid**. This is a grid search<sup>10</sup>, a brute force method that nevertheless parallelises well<sup>11</sup>. Once $S(\\boldsymbol{\\theta}_k)$ has been computed, it can be substituted into equation for the posterior to obtain the discretised probability density function. The coefficient $A$ is determined by numerical integration of $e^{-S(\\boldsymbol{\\theta})/2}$ using a suitable quadrature scheme. \n",
    "\n",
    "Computing $P(\\boldsymbol{\\theta}_k |\\tilde{y}_i)$  on a grid of parameters is **equivalent** to the assumption of a uniform prior: we are implicitly saying \"*the likelihood of $\\boldsymbol{\\theta}_k$ being the true parameter, $\\boldsymbol{\\theta}_0$, is related only to its goodness-of-fit with the data and not to any prior notions I have about what $\\boldsymbol{\\theta}_0$ should be.*\" Once $P(\\boldsymbol{\\theta}_k |\\tilde{y}_i)$ has been computed, it should be **inspected** to assess whether the parameter grid is sufficiently large: $P(\\boldsymbol{\\theta}_k |\\tilde{y}_i)$  should be **small** at the edges of the grid.\n",
    "\n",
    "<sup>10</sup> <sub> A grid search was used to generate contours of $S(\\boldsymbol{\\theta})$ for Figs. 1 and 2 in the Calibration notes, and some of the figures in the first two labs.</sub>\n",
    "\n",
    "<sup>11</sup> <sub> Computing $S(\\boldsymbol{\\theta}_1)$, which involves a forward run of the model for parameters $\\boldsymbol{\\theta}_1$, does not depend on knowing $S(\\boldsymbol{\\theta})$ at any other point. Contrast this to the gradient descent algorithm: we cannot compute $\\boldsymbol{\\theta}_4$ without first knowing $\\boldsymbol{\\theta}_3$.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 3.2.2\tMarginal parameter distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose there are two parameter – let’s consider $c$ and $k$ again, for the case of damped car suspension – in which case $P(c,k|\\tilde{y}_i)$ is a surface over 2D space. If we were able to determine the value of one of the parameters, say $c=c_0$, by other methods, the uncertain distribution of $k$ that remains is found by taking a slice through $P(c,k|\\tilde{y}_i)$ at $c=c_0$ (see Fig. 2). \n",
    "\n",
    "Another question we might ask runs along the lines: \"*what is the uncertain distribution of $k$ given the uncertainty of $c$?*\" or, alternatively \"*yes, $c$ is uncertain, but I don’t care; I just need to know the uncertainty of $k$.*\" This is called the **marginal distribution** of $k$ and is obtained by integrating $P(c,k|\\tilde{y}_i)$ over $c$\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(k|\\tilde{y}_i)=\\int P(c,k|\\tilde{y}_i)dc.\n",
    "\\end{equation}\t\n",
    "\n",
    "An illustration of marginal distributions and their relation to the posterior is shown in Fig. 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"img/fig2.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "**Figure 1: A sample objective function (left) over $c$-$k$ parameter space, and the corresponding posterior and marginal distributions (right). Note how $P(\\boldsymbol{\\theta}|\\tilde{y}_i)$ goes to zero at the edges of the grid, indicating the distribution is well resolved.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 A Linear Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Recall the example from Section 3.1.1.***\n",
    "\n",
    "Our group of human scientists have satisfied themselves that the physical process described previously is probably linear in nature and, therefore, a model of the form $y_i=mx_i+c$ will provide a suitable fit to the data. Using **gradient descent** calibration, they quickly determine that the best-fitting model has $m=2.2$ and $c=3.1$, i.e., $\\hat{\\boldsymbol{\\theta}}_0=[2.2,3.1]$. Clearly, this is different to the true value, $\\boldsymbol{\\theta}_0=[2.0,3.0]$ (Section 3.1.1), but that was expected.\n",
    "\n",
    "To gain a more in-depth understanding of the possible candidate models different from $\\hat{\\boldsymbol{\\theta}}_0$, a grid search is undertaken to first determine $S(\\boldsymbol{\\theta})$ and then subsequently compute a posterior, $P(\\boldsymbol{\\theta}|\\tilde{y}_i)$. The resulting distribution can be seen by running the cell below. Remember, the posterior distribution is making two important points:\n",
    "\n",
    "1. We don’t currently know, in fact we will never know, the true parameter, $\\boldsymbol{\\theta}_0$. Even our best guess, the least-squares fit, $\\hat{\\boldsymbol{\\theta}}_0$, will be wrong to some degree.\n",
    "2. Nevertheless, we can determine which parameter combinations are consistent with the data, and rank their likelihood relative to each other.\n",
    "\n",
    "Given these caveats, it is comforting to see that the true value, $\\boldsymbol{\\theta}_0$, is near enough to the centre of mass of $P(\\boldsymbol{\\theta}|\\tilde{y}_i)$ that it is among the likely candidates.\n",
    "\n",
    "***Execute the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nm = 41\n",
    "Nc = 41\n",
    "posterior(Nm,Nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The righthand figure is computed by a grid search over c-m parameter space.\n",
    "\n",
    "# Use the sliders to fit different models (left) and explore parameter space (right).\n",
    "\n",
    "# The best-fit model is not the same as the true process. Why?\n",
    "\n",
    "# Does the best-fit model or the true process correspond to the higher value on the righthand plot?\n",
    "\n",
    "# The righthand inset shows the posterior as a surface. The shape is 2D Gaussian.\n",
    "\n",
    "# If we were to fit a set of major and minor axes to the posterior, would they align with the c- and m- axes?\n",
    "\n",
    "# Change the values of Nm and Nc in the cell above and rerun. How many grid samples are required to \n",
    "# 'properly' estimate the posterior?\n",
    "\n",
    "# What tradeoffs are there in estimating the posterior?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the posterior - a 2D Gaussian distribution - is **rotated** with major and minor axes mis-aligned to the $c$- and $m$-axes. This indicates that the parameters $c$ and $m$ are **correlated** or, in this case, anti-correlated. In simpler terms, if we attempt to fit a steeper line (higher $m$) then we need a smaller intercept (smaller $c$) to obtain a reasonable match with the data. \n",
    "\n",
    "In the context of **inverse modelling**, where the entire exercise is motivated by a desire to determine $\\boldsymbol{\\theta}$, then the posterior parameter distribution (and the corresponding marginal distributions) communicate appropriately both our **best estimate** of $\\boldsymbol{\\theta}$ (the vector of values that maximise $P(\\boldsymbol{\\theta}|\\tilde{y}_i))$ as well as our **uncertainty**. However, when the goal is to make a prediction of the future, then model calibration is only an intermediate step. We must now consider how uncertainty in the parameters results in uncertainty in the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 A Geothermal Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Recall the example from Section 3.1.2.***\n",
    "\n",
    "Earlier we estimated the best fit values of $a$, $b$, and $c$ that parameterise a lumped parameter model of the Wairakei geothermal system. However, because the pressure data used to calibrate the model are uncertain, so to must be our estimates of the parameters.\n",
    "\n",
    "We will proceed by proposing a **prior distribution** for each of these parameters. As we have some information about the parameters - an idea of which values fit the data quite well - we will consider a normally distributed prior for each, centred on the best-fit value. \n",
    "\n",
    "\n",
    "***Execute the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpm_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the deviation slides to widen or narrow priors of a, b and c.\n",
    "\n",
    "# Add additional model samples by moving the 'samples' slider.\n",
    "\n",
    "# Obtain defensible prior distributions for 'a', 'b', and 'c'.\n",
    "\n",
    "# An implicit assumption of this approach is that 'a', 'b', and 'c' are uncorrelated.\n",
    "# What does this mean? (review the linear model example)\n",
    "\n",
    "# As more model samples are generated, the histograms on the RHS start to approximate the \n",
    "# theoretical distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3\tPrediction uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were **absolutely certain** that we had discovered the true values of all parameters, $\\boldsymbol{\\theta}_0$, then we could simply plug these into the forward model, $f(x_f;\\boldsymbol{\\theta}_0)$, to obtain a [rock solid prediction of the future](https://media.socastsrm.com/wordpress/wp-content/blogs.dir/697/files/2017/12/THE-ROCK-PRESIDENT-1.jpg), $y_f$. Unfortunately, when it comes to modelling, absolute certainty is in short supply. In fact, we saw in the previous section that, when observational error is allowed for, our best estimate, $\\hat{\\boldsymbol{\\theta}}_0$, is always going to be wrong to some degree, which means the prediction, $f(x_f;\\hat{\\boldsymbol{\\theta}}_0)$ **is always going to be wrong** as well. Oh dear. Not a great start.\n",
    "\n",
    "If we were absolutely **averse to risk**, then we might take a no-surprises approach and say \"*what are all the possible parameter sets, $\\boldsymbol{\\theta}$? Let’s predict the future for each of these, and then at least we’ll have an idea of the possible outcomes.*\" This is equivalent to sampling parameter sets from a uniform prior and yields, not one, but an [**ensemble of models**](https://xkcd.com/1885/) with an ensemble of outcomes. In the context of, say, climate modelling, this ensemble of outcomes might tell us that the 21st century could see anywhere between 5$^{\\circ}$C of warming and 2$^{\\circ}$C of cooling, although neither outcome is ranked more or less likely than the other. \n",
    "\n",
    "In fact though, we do know that some predictions are **more likely** to come true than others, because some of the parameter sets are more likely to be correct than others. For instance, suppose we have used some observations to compute a posterior distribution for the parameters, $P(\\boldsymbol{\\theta}|\\tilde{y}_i)$, from which we learn that $\\boldsymbol{\\theta}_1$ is twice as likely to be the true parameter set than $\\boldsymbol{\\theta}_2$. Then, we would imagine the outcome $y_{f,1}=f(x_f;\\boldsymbol{\\theta}_1)$ to, similarly, be twice as likely as the outcome $y_{f,2}=f(x_f;\\boldsymbol{\\theta}_2)$. We shall use this reasoning as the basis for constructing a **future forecast**<sup>12</sup>, which is simply a set of predictions with a probability attached to each.\n",
    "\n",
    "To construct the forecast, we take samples of possible parameter sets from the posterior distribution, $P(\\boldsymbol{\\theta}|\\tilde{y}_i)$ and then compute a prediction for each. The posterior is not uniform but rather is weighted toward the most-likely parameter combinations and, therefore, the predictions too will be **weighted** towards the most-likely outcomes. As more and more samples are computed, a histogram of the predictions – the forecast – will begin to resemble a probability distribution, $P(y_f|\\tilde{y}_i)$, which can be subsequently interrogated for its statistical properties. Of particular interest are the median outcome and an **interval** centred on it that encloses some substantial proportion of the outcomes<sup>13</sup>. \n",
    "\n",
    "**How many** samples from the posterior should be used to generate the forecast? The answer depends on how long the forward model takes to run, the number of computers available for running models<sup>14</sup>, and the diminishing returns in resolving $P(y_f |\\tilde{y}_i)$.\n",
    "\n",
    "<sup>12</sup> <sub> Here, we define the term forecast to mean a probabilistic prediction. However, take care, the terminology may mean different things to different people.</sub>\n",
    "\n",
    "<sup>13</sup> <sub> 90% is good number, although a high stakes outcome may call for a wider interval.</sub>\n",
    "\n",
    "<sup>14</sup> <sub> Like the grid search, this step parallelises well</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 A Linear Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Recall the example from Section 3.2.3.***\n",
    "\n",
    "The stupid humans wish to use their calibrated model and understanding of the posterior to make a prediction. For this rather contrived scenario, they will attempt to predict the physical process at $x_f=3.0$, which might be regarded as \"*about twice as far into the future than the period we have data for, [0-1]*\". The true future outcome is $y_{f,0}=f(x_f;\\boldsymbol{\\theta}_0 )=9$, but this is only learned long after the prediction is made. The prediction made using just the best-fit model is $y_f=f(x_f;\\hat{\\boldsymbol{\\theta}}_0)=9.75$. Not bad for mere humans! But also wrong.\n",
    "\n",
    "To be more rigorous, it is desirable to develop some sort of confidence in the prediction, $y_f$. As per Section 3.3, sample parameter sets are chosen<sup>15</sup> from the posterior distribution developed in Section 3.2.3, and a prediction is computed for each. Compiling a histogram of 2000 such samples – a forecast of the future – we can see that the 90% confidence interval is rather wide, [6, 13], but it **does contain** the true future outcome. \n",
    "\n",
    "<sup>15</sup> <sub> For this step, I fitted a multivariate normal distribution and sampled from it using the `scipy.stats.multivariate_normal` class in Python. In practical situations, the posterior may not have a nice shape and alternative sampling methods might be required (e.g., bootstrapping).</sub>\n",
    "\n",
    "***Execute the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var = 0.1\n",
    "prediction(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The figures above show:\n",
    "# (left) the modelled process\n",
    "# (middle) the sampled posterior \n",
    "# (right) the model forecast\n",
    "\n",
    "# Add some more samples using the slider.\n",
    "# How does the forecast change (width and shape) as you add more samples?\n",
    "\n",
    "# Extend the forecast out using the slider for xf.\n",
    "# How does the forecast change (width and shape) as you consider more distant futures?\n",
    "\n",
    "# Increase the variance on the observation by changing 'var' and rerunning the cell. Try 0.01 and 1.0.\n",
    "# How do the posterior and forecast change as you increase the observation error?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 A Geothermal Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Recall the example from Section 3.2.4.***\n",
    "\n",
    "Earlier, we estimated prior distributions for the model parameters, $a$, $b$, and $c$. To make a prediction of the future, we need only sample each of those distributions and pass those parameter values into the model. \n",
    "\n",
    "As an exercise, we shall attempt to \"predict\" the pressure in the reservoir in 2012. This is something the good folk back in 1981 may have tried to do when, say, reconsenting Wairakei for the next 30 years.\n",
    "\n",
    "Of course, since we actually have pressure measurements up to 2012, we'll be able to verify our forecast...\n",
    "\n",
    "***Execute the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sa = 1.1e-4\n",
    "sb = 5.5e-3\n",
    "sc = 1.7e-3\n",
    "lpm_prediction(sa,sb,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prior width values sa, sb and sc in the cell above, then execute. You should obtain\n",
    "# these values from the exercise in Section 3.2.4, see the TITLE of that plot for values.\n",
    "\n",
    "# The lefthand figure samples the prior and extrapolates models out to 2012 (using the \n",
    "# actual flow-rate, bit of an inconsistency there...)\n",
    "\n",
    "# The righthand figure shows a forecast of reservoir pressure in 2012.\n",
    "\n",
    "# Use the slider bar to add more models. How does the shape and width of the forecast change?\n",
    "\n",
    "# Click the \"reveal future\" button to compare your forecast against the actual data. \n",
    "# How did you do?\n",
    "\n",
    "# Speculate on any discrepancy between your forecast and the actual outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3.4\tStructural error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While definitions of **structural (or model) error** can be a little grey, here we are referring to any item falling under the rather broad umbrella of \"*improper model formulation*\" or \"*deviations between model and reality*\". This includes all model assumptions related to: \n",
    "- The governing physics, e.g., the differential equations being solved neglect gravity, which in practice has some influence on the outcome, but in this case is **assumed to be minor**.\n",
    "- Time and space discretization, e.g., to solve the problem in a reasonable amount of time, we often must solve on a grid comprising a finite number of points and advancing the solution with finite sized time steps. Anything in between might be inferred by an interpolation scheme, but is **not explicitly captured** by the model.\n",
    "- Represented features and homogenization. For instance, in a continuum model we might choose to **exclude** a finite-sized feature that has different properties to the rest of the continuum (an example from geothermal reservoir modelling is to not include a fault in the model). In practice, no continuum is perfectly homogeneous in its properties – all materials contain defects, imperfections or innate variability. However, characterizing and modelling heterogeneous materials is a non-trivial undertaking.\n",
    "\n",
    "Assessment of the size or impact of structural error often amounts to assessing the consequences of model assumptions (see Design notes, Section 1.4.3). In some cases, this requires running the model with the assumption relaxed and satisfying oneself that the behaviour or predictions are not substantially altered.\n",
    "\n",
    "The above assumptions – in fact any assumption made consciously, articulated or not – fall into a category of ignorance termed **known unknowns**. These are the things we know we don’t know or, as James Clerk Maxwell<sup>16</sup> put it, \"*thoroughly conscious ignorance*\". Observation error, parameter and prediction uncertainty also fall under this heading.\n",
    "\n",
    "More difficult to deal with are the **unknown unknowns**, i.e., the things we don’t know that we don’t know<sup>17</sup>. This includes the physical processes we’re not aware are operating or that we have interpreted incorrectly, important features or structures that cannot be directly observed (again, unknown faults and fractures are a big challenge in modelling reservoirs), or bad information we have been provided that we actually think is good (data, expert knowledge). There is little that can be done to guard against unknown unknowns beyond a rigorous, professional approach to problem solving, i.e., gathering data, considering different modelling options, providing impartial self-critique, and maintaining an open-minded, but sceptical, outlook.\n",
    "\n",
    "<sup>16</sup> <sub> 19th century titan of physics, populariser of dimensional analysis, originator of the hipster beard.</sub>\n",
    "\n",
    "<sup>17</sup> <sub> When you are first born, everything is an unknown unknown, which perhaps explains why infants find mathematical modelling quite challenging, hyuk hyuk hyuk.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.4.1 A Linear Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***Recall the example from Section 3.3.1.***\n",
    "\n",
    "Another group of scientists have developed a different model. They know without a doubt<sup>18</sup> that the true process is logarithmic and is described by a relationship of the form: $y_i=a\\,\\ln⁡(x_i-x_0)+b$. Having three parameters instead of two, their model is clearly more sophisticated than the linear one<sup>19</sup>. Undertaking a similar analysis of the data, $\\tilde{y}_i$, they compute a best-fit model, compute the posterior parameter distribution and sample from it to obtain a forecast of the future (Sections 3.2-3.3). This is implemented in the cell below, alongside the forecast from the first group, who used a linear model. The 90% forecast interval for the logarithmic model excludes the true outcome by a large margin.\n",
    "\n",
    "How did the second group get it so wrong? They followed the same procedure as the first group: fitting a model to the data, exploring parameter space to construct a posterior, and then sampling from it to construct a forecast. Indeed, with respect to the observations, $\\tilde{y}_i$, the best-fit logarithmic model has a smaller objective function ($S(\\hat{\\boldsymbol{\\theta}}_0 )=4.2$) than the best-fit linear model ($6.5$), and on that basis we might prefer the former.\n",
    "\n",
    "The error they made occurred early on during model selection when the underlying physics were assumed to be logarithmic in nature. The particular choice may have been motivated by the data (\"*the logarithmic model fits $\\tilde{y}_i$ better*\") or by an incorrect analysis of the physics. In either case, structural error was introduced during model development that, in the end, had a deleterious effect of future predictions.\n",
    "\n",
    "<sup>18</sup> <sub> \"*It is very difficult to find a black cat in a dark room. Especially when there is no cat.*\" - [Proverb](https://i.kym-cdn.com/photos/images/original/001/153/173/152.jpg).</sub>\n",
    "\n",
    "<sup>19</sup> <sub> Albeit, less correct.</sub>\n",
    "\n",
    "***Execute the cell below and answer the questions (be patient, this one takes a few seconds to setup).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "structural()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The cell above offers three alternative models (including the log model described above).\n",
    "# - Switch between the models using the dropdown menu.\n",
    "# - Zoom in to see how each fits the data.\n",
    "# - Use the slider to change the prediction window.\n",
    "\n",
    "# Rank the FOUR models (incl. linear) in terms of fit to the data.\n",
    "# (the legend on the lefthand figures gives the obj. func. value S)\n",
    "\n",
    "# If 'ability to fit the data' was the ONLY criteria to choose a model,\n",
    "# which would you choose?\n",
    "\n",
    "# Of the three competing models, which ones successfully 'predict' the future at xf = 3?\n",
    "\n",
    "# Describe the magnitude of the prediction error for alternative models as xf increases.\n",
    "\n",
    "# What (real-world) actions could you take to help choose between competing models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.4.2 A Geothermal Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***Recall the example from Section 3.3.2.***\n",
    "\n",
    "Sometimes, a self-proclaimed \"expert\" will assert that they **know** the value of a parameter. Maybe they do (to within some degree of accuracy). Maybe they don't and they're just an overconfident blowhard. Most often, they don't, but they're a pragmatist that realises not *everything* can be treated as a free parameter for calibration.\n",
    "\n",
    "Nevertheless, as soon as you fix a parameter to an incorrect value, you will introduce **structural error** into a model. Let's look at an example involving calibration and prediction.\n",
    "\n",
    "Suppose we \"know\" the value of the parameter $c$ for the lumped parameter model. Great! Now we only need to calibrate two other parameters, $a$ and $b$, which won't take nearly as long.\n",
    "\n",
    "Unfortunately, we picked the wrong value for $c$. Which means that, during calibration, values of $a$ and $b$ will end up **taking on best-fit values that are different to what they would otherwise have been** had we used a different value of $c$. This will ultimately affect our ability to make a prediction of the future (and to correctly characterise $a$ and $b$!)\n",
    "\n",
    "***Execute the cell below and answer the questions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lpm_structural()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The code above takes a FIXED value of c, and then calculates a and b\n",
    "# that best fit the data. Use the slider to change the input value of c.\n",
    "\n",
    "# How does the quality of the calibration change?\n",
    "\n",
    "# How do the parameter values a and b change?\n",
    "\n",
    "# How does the prediction of the future change?\n",
    "\n",
    "# How does making a decision to fix a particular parameter value for c affect our ability\n",
    "# to make a prediction of the future?\n",
    "\n",
    "# We can think about calibration as an exercise in inverse modelling. That is, we are\n",
    "# seeking the \"true\" values of 'a' and 'b' because they're too difficult to measure by\n",
    "# other (more direct) means.\n",
    "\n",
    "# How does making a decision to fix a particular parameter value for c affect our ability\n",
    "# to do inverse modelling?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading for the interested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firestein, S (2012). *Ignorance: How it drives science*. Oxford University Press.\n",
    "\n",
    "Marzocchi, W., and J. D. Zechar (2011). *Earthquake Forecasting and Earthquake Prediction: Different Approaches for Obtaining the Best Model*. Seismological Research Letters 82, 442-448.\n",
    "\n",
    "Tarantola, A. (2006). *Popper, Bayes and the inverse problem*. Nature Physics 2, 492-494.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation (MLE) is a method for estimating the parameters of a statistical model. Central to MLE is the concept of observations, $\\tilde{y}_i$, which are drawn from a probability density function, $P(⋅;\\boldsymbol{\\theta})$, that has parameters, $\\boldsymbol{\\theta}$. The true function is denoted $P_0(⋅;\\boldsymbol{\\theta}_0)$ and the goal of MLE is to find an estimate of $\\boldsymbol{\\theta}_0$. \n",
    "In our case, the act of making an observation is equivalent to sampling a normally distributed random error (variance, $\\sigma_i^2$) and adding it to the true model value (mean, $f(x_i;\\boldsymbol{\\theta})$), therefore the density function for the observation $\\tilde{y}_i$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\tP(\\tilde{y}_i|\\boldsymbol{\\theta})=\\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp ⁡\\left[ -\\frac{\\left(\\tilde{y}_i-f(x_i;\\boldsymbol{\\theta})\\right)^2}{2\\sigma_i^2}\\right],\n",
    "\\end{equation}\t\n",
    "\n",
    "which clearly depends on $\\boldsymbol{\\theta}$.\n",
    "\n",
    "For $n$ observations, one forms the joint density function by multiplying together the equation above for each observation, i.e., \n",
    "\n",
    "\\begin{equation}\n",
    "\tP(\\tilde{y}_1,\\tilde{y}_2,\\dots\\tilde{y}_n│\\boldsymbol{\\theta})=\\prod\\limits_{i=1}^n P(\\tilde{y}_i |\\boldsymbol{\\theta}) .\n",
    "\\end{equation}\t\n",
    "\n",
    "The goal is to find $\\hat{\\boldsymbol{\\theta}}_0$ that maximises this quantity, or rather its logarithm, which we term the log-likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "\t\\ln ⁡L=\\sum\\limits_{i=1}^n -\\frac{(\\tilde{y}_i-f(x_i;\\boldsymbol{\\theta}))^2}{2\\sigma_i^2}+\\text{const}.\n",
    "\\end{equation}\t\n",
    "\n",
    "We identify the equation above as the objective function defined in Section 3.2 (to within a multiplicative factor and a constant), and thus we can write $P(\\tilde{y}_1,\\tilde{y}_2,\\dots \\tilde{y}_n│\\boldsymbol{\\theta}) = A \\exp⁡(-S(\\boldsymbol{\\theta})/2)$.\n",
    "\n",
    "Bayes theorem for multiple parameters and observations is written\n",
    "\n",
    "\\begin{equation}\n",
    "\tP(\\boldsymbol{\\theta}│\\tilde{y}_1,\\tilde{y}_2,\\dots\\tilde{y}_n)=\\frac{P(\\tilde{y}_1,\\tilde{y}_2,\\dots\\tilde{y}_n│\\boldsymbol{\\theta})  P(\\boldsymbol{\\theta})}{P(\\tilde{y}_1,\\tilde{y}_2,\\dots\\tilde{y}_n )},\t\n",
    "\\end{equation}\n",
    "\n",
    "which can be read literally \"*the probability of the parameters ($\\boldsymbol{\\theta}$) given the data $(\\tilde{y}_1,\\tilde{y}_2,\\dots\\tilde{y}_n)$ is equal to the probability of the data given the parameters (the joint density) multiplied by the probability of the parameters (the prior) divided by the probability of the data averaged over all parameters.*\" For a uniform prior, $P(\\boldsymbol{\\theta})$ and $P(\\tilde{y}_1,\\tilde{y}_2,\\dots\\tilde{y}_n)$ are constant. Hence, the posterior $P(\\boldsymbol{\\theta}│\\tilde{y}_1,\\tilde{y}_2,\\dots\\tilde{y}_n)$ is the likelihood function computed above.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
