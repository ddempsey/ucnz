{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581effe1",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\">Machine Learning and Artificial Intelligence</div>\n",
    "\n",
    "<div style=\"text-align: center\"> <sub>ENCN404 - Applied Modelling and AI</sub></div>\n",
    "\n",
    "$\\,$\n",
    "\n",
    "<div style=\"text-align: center\"> University of Canterbury </div>\n",
    "\n",
    "$\\,$\n",
    "\n",
    "<img src=\"img/ml.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Notebook instructions\n",
    "\n",
    "Run cells containing code by clicking on them and hitting **Ctrl+Enter** or by Cell>Run Cells in the drop-down menu.\n",
    "\n",
    "For queries, contact the course instructor or notebook author (David Dempsey)\n",
    "\n",
    "#### Contents\n",
    "*Explorative AI*\n",
    "1. Data Exploration with Pandas\n",
    "2. Feature Engineering\n",
    "3. Unsupervised Learning and Clustering\n",
    "4. Hypothesis Testing\n",
    "\n",
    "*Predictive AI*\n",
    "\n",
    "5. Supervised Learning\n",
    "6. Learning Algorithms\n",
    "7. Performance Metrics\n",
    "8. Cross Validation\n",
    "\n",
    "*Advanced Topics*\n",
    "\n",
    "9. Deep Learning\n",
    "10. Physics-informed Machine Learning\n",
    "11. Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea250d",
   "metadata": {},
   "source": [
    "# Explorative AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107522b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d83fd",
   "metadata": {},
   "source": [
    "## 1. Data Exploration with Pandas\n",
    "\n",
    "Work through the examples below during the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bdc526",
   "metadata": {},
   "source": [
    "### 1.1 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3168d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The most important object is the DataFrame. Think of this like a table in a spreadsheet.\n",
    "data={'time': [30, 60, 90, 120, 150], 'rainfall': [4, 11, 32, 8, 0], 'runoff': [0, 0, 1.7, 8.6, 3.1]}\n",
    "\n",
    "# create the dataframe from a dictionary of data\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "# look at the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2050062",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c356c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display the row and column counts\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b79abf3",
   "metadata": {},
   "source": [
    "### 1.2 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8f25bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dataframes have indices. These are like the indices of an array or list, e.g., 0, 1, 2, … -1. \n",
    "# The indices populate by default in the Python convention. They can be accessed from the 'index' attribute.\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c45ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Indices don't have to be integers. We can change them to something else. \n",
    "# A popular choice is some kind of measure of time, in which case we are working with time series data.\n",
    "df.set_index('time', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1a206",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can use indices to get access to parts of the dataframe.\n",
    "print(df.loc[30])\n",
    "print(df.loc[90,'rainfall'])\n",
    "print(df.loc[120:,'runoff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f5f47",
   "metadata": {},
   "source": [
    "### 1.3 Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301dbc5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# extract a series (one column) from the larger dataframe\n",
    "rain=df['rainfall']\n",
    "\n",
    "# summarize aspects of the series\n",
    "print(rain.max())             # or min, mean, std, sum\n",
    "print(rain.describe())\n",
    "print(rain.unique())          # sort_values, value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6c7a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# With matplotlib, we can also generate plots\n",
    "rain.plot(kind='box')   # or line, box, pie…\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a89947",
   "metadata": {},
   "source": [
    "### 1.4 Other operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279bc708",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We'll use the dataframe as a variable on which to do calculations (like a spreadsheet). \n",
    "# For example, calculate new columns\n",
    "df['rnf_rnd']=df['runoff'].round()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2d38f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# or calculate a summary row\n",
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bccc2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can write dataframes out to files, and read them back in again. We'll generally use CSV files.\n",
    "df.to_csv('rainfall.csv')\n",
    "df2=pd.read_csv('rainfall.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda4c8ac",
   "metadata": {},
   "source": [
    "### 1.5 Rolling window calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc023e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rolling window calculations are a useful series operation\n",
    "df['avg_rain']=df['rainfall'].rolling(3).mean()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb52e96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rolling() can be chained with apply() to any function you can think of, e.g., sum of squares\n",
    "def sum_of_squares(x):\n",
    "    return np.sum(x**2)\n",
    "df['ss_runoff2']=df['runoff'].rolling(3).apply(sum_of_squares)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59710d31",
   "metadata": {},
   "source": [
    "### 1.6 Grouping and sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db057d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's look now at some categorical data in buildings.csv\n",
    "df=pd.read_csv('buildings.csv')\n",
    "df.set_index('Building', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6cee4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can sort on a particular column\n",
    "df.sort_values('Cost', ascending=False)\n",
    "\n",
    "# (Note: sort_values() OUTPUTS a new sorted dataframe. It does not sort the original\n",
    "#  dataframe unless you set inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb5079",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can group according to a category and calculate summaries of those groups\n",
    "df.groupby('Type').median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0953f97",
   "metadata": {},
   "source": [
    "### 1.7 Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e43495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can use pandas to find and replace outliers, for example in the data below\n",
    "df=pd.DataFrame({'disp': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.2, 10, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8], 'load': [10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]}) \n",
    "plt.plot(df['load'], df['disp'], 'kx'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c6e7e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate a new column that is the z-score (deviation from the mean)\n",
    "df['zsc']=(df['disp']-df['disp'].mean())/df['disp'].std()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1604e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find outliers based on large absolute zscores\n",
    "no_outliers=df['disp'].where(df['zsc'].abs()<3)\n",
    "\n",
    "# replace the outliers with linear interpolation\n",
    "df['disp']=no_outliers.interpolate(method='linear') \n",
    "plt.plot(df['load'], df['disp'], 'kx'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39056d96",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Work through the examples below in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784fa9c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660466c",
   "metadata": {},
   "source": [
    "### 2.1 Standardization and normalization\n",
    "\n",
    "Some ML algorithms struggle with data that have order of magnitude different scales and ranges. Z-score standardization transforms these to distributions with zero mean and unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66587d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displacement data and calculated strain\n",
    "u = np.array([0.011, 0.012, 0.014, 0.022, 0.045, 0.123, 0.190, 0.231, 0.245, 0.249, 0.251])   # 2.5 m samples\n",
    "e = np.diff(u)/2.5  # strain\n",
    "df = pd.DataFrame({'displacement': u[:-1], 'strain': e})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550776aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note how the blue displacement dominates the plot, whereas red strain is less distinctive\n",
    "# this is because of their different ranges of values\n",
    "x=np.arange(len(e))*2.5\n",
    "plt.plot(x, df['displacement'], 'b-', label='u')\n",
    "plt.plot(x, df['strain'], 'r-', label='e')\n",
    "plt.gca().set_xlabel('x'); plt.gca().set_ylabel('data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29dcb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale the data to a unit normal distribution\n",
    "scaled_data = scaler.fit_transform(df[['displacement', 'strain']])\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=['scaled_displacement', 'scaled_strain'])\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b789c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see how shapes of the data are preserved, but they have been rescaled to have similar range and centre\n",
    "plt.plot(x, scaled_df['scaled_displacement'], 'b-', label='u')\n",
    "plt.plot(x, scaled_df['scaled_strain'], 'r-', label='e')\n",
    "plt.gca().set_xlabel('x'); plt.gca().set_ylabel('scaled data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c38855f",
   "metadata": {},
   "source": [
    "### 2.2 One-hot encoding\n",
    "\n",
    "Convert categorical data into binary input features, e.g., commercial/not commercial, residential/not residential. This feature is easier for some ML algorithms to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca18c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df = pd.DataFrame({'Type': ['Resid', 'Comm', 'Indus', 'Resid', 'Indus', 'Comm']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152aa4c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(df[['Type']])\n",
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed323a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe with the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Type']))\n",
    "# Concatenate the original and encoded dataframes\n",
    "result_df = pd.concat([df, encoded_df], axis=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00dfa9",
   "metadata": {},
   "source": [
    "### 2.3 Rolling window features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9e6a1",
   "metadata": {},
   "source": [
    "These are constructed from **WINDOWS** that slide over time series data. A new feature value is calculated by applying some **FUNCTION** to the data contained within the window. When the window moves on, the function is reapplied and a new feature value is calculated. \n",
    "\n",
    "These features highlight certain characteristics of the data by supressing noise components not related to that characteristic.\n",
    "\n",
    "In this example, we have a time series of traffic density at an intersection over two years. The change over time is complex, with a strong first-order trend, overprinted by second-order patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911ed0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read in some data\n",
    "df=pd.read_csv('traffic_data.csv',parse_dates=[0]).set_index('time')\n",
    "ts=df['density']\n",
    "\n",
    "# plot the raw data\n",
    "f,(ax1,ax2,ax3)=plt.subplots(1,3, figsize=(16,4))\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ts.plot(style='k-', lw=0.5, ax=ax, label='raw data')\n",
    "\n",
    "# Calculate a 150-day rolling mean\n",
    "Tm=150      \n",
    "df['density_mean']=ts.rolling(window=Tm).mean()\n",
    "df['density_mean'].plot(style='b',ax=ax1)\n",
    "\n",
    "# Calculate a 30-day rolling standard deviation\n",
    "Tsd=30\n",
    "df['density_std']=ts.rolling(window=Tsd).std()\n",
    "df['density_std'].plot(style='b', ax=ax2.twinx())\n",
    "\n",
    "# Calculate a rolling X-day harmonic \n",
    "def rolling_fft(x, ti):\n",
    "    fft = np.fft.fft(x)/len(x)\n",
    "    psd = np.abs(fft)**2/2\n",
    "    period_of_interest = ti\n",
    "    ts=1./(np.fft.fftfreq(len(x)))\n",
    "    i=np.argmin(abs(ts-ti))\n",
    "    return psd[i]\n",
    "\n",
    "Ti=30   # harmonic (days)\n",
    "ts.rolling(window=120).apply(rolling_fft, args=(Ti,)).plot(style='b', ax=ax3.twinx())\n",
    "\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set_ylabel('traffic density')\n",
    "ax1.set_title(f'feature 1: {Tm:d}-day average')\n",
    "ax2.set_title(f'feature 2: {Tsd:d}-day std. dev.')\n",
    "ax3.set_title(f'feature 3: {Ti:d}-day harmonic')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16d90d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from encn404 import rolling_window\n",
    "rolling_window()\n",
    "# run this cell and answer the questions below:\n",
    "# For feature 1, slide the WINDOW length from 150 to 20 days. How does the feature change?\n",
    "# Set the SAME window length for features 1 and 2. How are they different?\n",
    "# For feature 3, what information is it extracting?\n",
    "# Which feature extracts the first-order trend?\n",
    "# Which feature identifies second-order characteristics that overprint the trend?\n",
    "# which of these first- or second-order characteristics are signal and which are noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7042b48",
   "metadata": {},
   "source": [
    "### 2.4 Interaction features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ffbf1a",
   "metadata": {},
   "source": [
    "We seek to **combine** different types of data together into new features that may better capture characteristics of the covarying data. For example, the product of two data types. Or a ratio.\n",
    "\n",
    "In this example, we will suppose that we have strength measurements for concrete samples with different amounts of water and cement, and that have been aged for different lengths of time. Data columns are then Cement (C), Water (W) and Age (A), whereas interaction features could be **ratios** or **products**, e.g., C/W (cement-water ratio), C\\*A (cement-age product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf020d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in Cement (C), Water (W) and Age (A) data for concrete\n",
    "df=pd.read_csv('concrete_data.csv')\n",
    "print(df.head())\n",
    "\n",
    "# the Strength data will be our 'label' (more on this later)\n",
    "y=df['Strength']\n",
    "df=df.drop(columns=['Strength'])\n",
    "\n",
    "# plot correlations between the raw data and strength\n",
    "f,axs=plt.subplots(1,3,figsize=(14,4))\n",
    "for ax,col in zip(axs, df.columns[:3]):\n",
    "    ax.plot(df[col], y,'bo',alpha=0.7)\n",
    "    ax.set_xlabel(col)\n",
    "axs[0].set_ylabel('Strength')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1332ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create Polynomial Interaction Features with degree 2, e.g., C*A, W*C\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "int_features = poly.fit_transform(df)\n",
    "\n",
    "# get feature names and create a dataframe\n",
    "feature_names = poly.get_feature_names_out(df.columns)\n",
    "df_int = pd.DataFrame(int_features, columns=feature_names)\n",
    "df_int.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff378325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the new interaction features and look for correlations\n",
    "f,axs=plt.subplots(1,6,figsize=(16,3))\n",
    "for ax,col in zip(axs, df_int.columns[-6:]):\n",
    "    ax.plot(df_int[col], y, 'bo', alpha=0.7)\n",
    "    ax.set_xlabel(col)\n",
    "axs[0].set_ylabel('Strength')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69898383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extend the dataframe with reciprocals, i.e., 1/A, 1/C, 1/W\n",
    "df2 = pd.DataFrame(1./df.values, columns=[f'{col}^-1' for col in df.columns])\n",
    "df3 = pd.concat([df,df2], axis=1)\n",
    "\n",
    "# interaction features now also include ratios, e.g., C/W\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "int_features = poly.fit_transform(df3)\n",
    "feature_names = poly.get_feature_names_out(df3.columns)\n",
    "df_int2 = pd.DataFrame(int_features, columns=feature_names)\n",
    "df_int2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdda07c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the new interaction features and look for correlations\n",
    "f,axs=plt.subplots(4,5,figsize=(16,10))\n",
    "axs=[item for sublist in axs for item in sublist]\n",
    "for ax,col in zip(axs, df_int2.columns[-20:]):\n",
    "    ax.plot(df_int2[col], y,'bo',alpha=0.7,label=col)\n",
    "    ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a672c4",
   "metadata": {},
   "source": [
    "### 2.5 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477723a",
   "metadata": {},
   "source": [
    "PCA is a statistical procedure that helps us work with **correlated** data or features. It converts variables that appear related to each other into new sets of uncorrelated variables called principal components. These components capture the **largest directions of variation** within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2e24b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in Cement (C), Water (W) and Age (A) data for concrete\n",
    "df=pd.read_csv('concrete_data.csv')\n",
    "print(df.head())\n",
    "\n",
    "# the Strength data will be our 'label' (more on this later)\n",
    "y=df['Strength']\n",
    "df=df.drop(columns=['Strength'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c757d1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate predictors (X) and label (y)\n",
    "X = df\n",
    "\n",
    "# Standardize the predictors so they contribute equally\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=3)  # Reduce to 3 components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "weights = pca.components_\n",
    "\n",
    "# Create a new dataframe with the PCA components\n",
    "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca = pd.concat([df,df_pca], axis=1)\n",
    "\n",
    "# Print the principal component weights and variance\n",
    "for i in range(3):\n",
    "    w=weights[i,:]\n",
    "    s=f'PC{i+1:d}='\n",
    "    for j,c in enumerate(X.columns):\n",
    "        s+=f'{w[j]:3.2f}*{c}+'\n",
    "    print(s[:-1]+f',\\tExplained variance={int(pca.explained_variance_ratio_[i]*100):d}%')\n",
    "\n",
    "# plot the new principal components and look for correlations\n",
    "f,axs=plt.subplots(1,6,figsize=(16,3))\n",
    "for ax,col in zip(axs, df_pca.columns):\n",
    "    ax.plot(df_pca[col], y, 'bo', alpha=0.7)\n",
    "    ax.set_xlabel(col)\n",
    "axs[0].set_ylabel('Strength')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1cd2e",
   "metadata": {},
   "source": [
    "## 3. Unsupervised Learning and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ddc06d",
   "metadata": {},
   "source": [
    "### 3.1 Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab38e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from encn404 import clustering\n",
    "clustering()\n",
    "# run this cell and answer the questions below:\n",
    "# in step 0, how should we guess initially at the centroids?\n",
    "# in step 1, how is cluster membership determined?\n",
    "# in step 2, how is the new centroid position calculated?\n",
    "# in step 3, how and why does cluster membership change?\n",
    "# when does the algorithm stop?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f35551",
   "metadata": {},
   "source": [
    "### 3.2 K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4321ab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from encn404 import kmeans\n",
    "kmeans()\n",
    "# run this cell and answer the questions below:\n",
    "# In step 0, how many clusters are there? How do you \"know\" this answer?\n",
    "# In step 1, there are two clusters. Which one has the better centroid? \n",
    "# Does the centroid location, relative to the data, tell us about how good the data are?\n",
    "# In step 2, we have selected the right number of clusters. But how would an algorithm know this?\n",
    "# What is the effect of increasing cluster number beyond what is required by the data?\n",
    "# How is the Sihouette Score calculated? Ask an AI.\n",
    "# How can we use the Silhouette Score to determine what the best number of clusters is for our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6f82f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports from sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# make up some data\n",
    "X, y = make_blobs(n_samples=300, n_features=2, centers=3, random_state=42)\n",
    "\n",
    "# create and fit the Kmeans object\n",
    "kmns = KMeans(n_clusters=3, random_state=42)\n",
    "kmns.fit(X)\n",
    "\n",
    "# extract the key clustering outputs\n",
    "centroids= kmns.cluster_centers_\n",
    "membership = kmns.predict(X)\n",
    "ss = silhouette_score(X, membership)\n",
    "inertia = kmns.inertia_\n",
    "\n",
    "# plot the clusters\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=membership, cmap='viridis', marker='o', edgecolor='k', s=50)\n",
    "plt.scatter(kmns.cluster_centers_[:, 0], kmns.cluster_centers_[:, 1], c='red', marker='x', s=100, label='Cluster centers')\n",
    "plt.title(f\"KMeans Clustering (k=4), inertia={inertia:3.2f}\"); plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a360e",
   "metadata": {},
   "source": [
    "### 3.3 DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb8c16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# make clusters\n",
    "X, y = make_blobs(n_samples=200, centers=2, cluster_std=1.5, random_state=42)\n",
    "\n",
    "# add nosie\n",
    "np.random.seed(42)\n",
    "X = np.vstack([X, np.random.rand(50, 2) * 14 - 5])\n",
    "\n",
    "# Apply DBSCAN algorithm\n",
    "eps = 2.0\n",
    "min_samples = 5\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "y_dbscan = dbscan.fit_predict(X)\n",
    "\n",
    "# Plot the clusters\n",
    "f,(ax1,ax2)=plt.subplots(1,2,figsize=(10, 4))\n",
    "ax1.plot(X[:, 0], X[:, 1], 'o', color=[0.7,0.7,0.7], ms=7, mec='k')\n",
    "unique_labels = np.unique(y_dbscan[y_dbscan != -1])\n",
    "for label,c in zip(unique_labels,['y','m']):\n",
    "    ax2.plot(X[y_dbscan == label, 0], X[y_dbscan == label, 1], c+'o', mec='k', ms=7, label=f\"Cluster {label}\")\n",
    "ax2.scatter(X[y_dbscan == -1, 0], X[y_dbscan == -1, 1], color='gray', s=50, label=\"Noise\")\n",
    "ax2.set_title(\"DBSCAN Clustering\")\n",
    "ax2.set_xlabel(\"Feature 1\")\n",
    "ax2.set_ylabel(\"Feature 2\")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd289d17",
   "metadata": {},
   "source": [
    "## 4. Hypothesis Testing\n",
    "\n",
    "Work through the examples below during the lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ef082",
   "metadata": {},
   "source": [
    "### 4.1 T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae9037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# Data: compressive strength measurements of two concrete types\n",
    "concrete_type_A = [30, 32, 31, 33, 29, 28, 30, 31, 32, 30]\n",
    "concrete_type_B = [35, 34, 33, 36, 32, 31, 33, 34, 35, 33]\n",
    "\n",
    "# Perform an independent two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(concrete_type_A, concrete_type_B)\n",
    "\n",
    "# Set significance level (alpha)\n",
    "alpha = 0.05\n",
    "if p_value < alpha: \n",
    "    print(p_value, 'different means')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac8e34",
   "metadata": {},
   "source": [
    "### 4.2 Kendall's Rank Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ed8a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data: stage and flow measurements on a river\n",
    "stage = [1.2, 1.5, 1.8, 2.0, 2.3, 2.6, 2.9, 3.2, 3.5, 3.8]\n",
    "flow = [11.0, 11.3, 12.0, 12.3, 18.0, 18.5, 19.8, 25.3, 28.3, 28.2]\n",
    "\n",
    "# Calculate Kendall's tau and p-value\n",
    "tau, p_value = stats.kendalltau(stage, flow)\n",
    "\n",
    "# - A positive tau indicates a positive correlation (as stage height increases, river flow tends to increase).\n",
    "# - The p-value tells us if the correlation is statistically significant.\n",
    "tau, p_value = stats.kendalltau(stage[::3], flow[::3])\n",
    "print(tau, p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4d24b",
   "metadata": {},
   "source": [
    "### 4.3 Mann-Whitney U test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b446b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform the Mann Whitney U test\n",
    "statistic, p_value = stats.mannwhitneyu(concrete_type_A, concrete_type_B, alternative='two-sided')\n",
    "\n",
    "# Set significance level (alpha)\n",
    "alpha = 0.05\n",
    "if p_value < alpha: \n",
    "    print('different medians')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc8a8e2",
   "metadata": {},
   "source": [
    "# Predictive AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b59788",
   "metadata": {},
   "source": [
    "## 5. Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900898e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports and data definitions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize     # find parameters that minimize the input\n",
    "from functools import partial           # locks the first N inputs to the function to particular values\n",
    "from sklearn.linear_model import LinearRegression     # multi-variate linear model\n",
    "X=np.array([[1.87,6.66,0.78,8.34],[4.75,2.99,0.78,5.91]])\n",
    "y=np.array([15.28,17.04,3.85,17.52])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df90793",
   "metadata": {},
   "source": [
    "### 5.1 Model 1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40e20b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the training set (three data points)\n",
    "rain=X[0,:3]\n",
    "runoff=y[:3]\n",
    "\n",
    "# find the best-fitting linear model\n",
    "    # define the loss function\n",
    "def sum_of_squared_differences(feature, runoff, pars):\n",
    "    m,c=pars\n",
    "    return np.sum([(runoff_i-m*feature_i-c)**2 for runoff_i,feature_i in zip(runoff,feature)])\n",
    "    # training\n",
    "m_best, c_best=minimize(partial(sum_of_squared_differences, rain, runoff), [1,1]).x\n",
    "\n",
    "# show the result\n",
    "f,ax=plt.subplots(1,1,figsize=(6,3))\n",
    "ax.plot(X[0,:], y, 'o', ms=5, color=[1., 0.5, 0.5], label='all')\n",
    "ax.plot(X[0,:3], y[:3], 'o', ms=3, color=[0.2, 0.2, 0.8], label='train')\n",
    "ax.plot([0,10], [c_best,m_best*10+c_best], ':', color=[0.2, 0.2, 0.8], label=f'y={m_best:3.2f}x+{c_best:3.2f}')\n",
    "ax.set_xlim([0,10]);ax.set_xlabel('rain')\n",
    "ax.set_ylim([0,25]);ax.set_ylabel('runoff')\n",
    "ax.set_title(f'sum of squares = {sum_of_squared_differences(X[0,:], y, [m_best, c_best]):3.2f}')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894890f",
   "metadata": {},
   "source": [
    "### 5.2 Model 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2755be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the training set (all data points)\n",
    "rain=X[0,:]\n",
    "runoff=y\n",
    "\n",
    "# find the best-fitting linear model\n",
    "    # use loss function defined above\n",
    "    # training\n",
    "m_best, c_best=minimize(partial(sum_of_squared_differences, rain, runoff), [1,1]).x\n",
    "\n",
    "# show the result\n",
    "f,ax=plt.subplots(1,1,figsize=(6,3))\n",
    "ax.plot(X[0,:], y, 'o', ms=5, color=[1., 0.5, 0.5], label='all')\n",
    "ax.plot(X[0,:], y[:], 'o', ms=3, color=[0.2, 0.2, 0.8], label='train')\n",
    "ax.plot([0,10], [c_best,m_best*10+c_best], ':', color=[1, 0.5, 0.5], label=f'y={m_best:3.2f}x+{c_best:3.2f}')\n",
    "ax.set_xlim([0,10]);ax.set_xlabel('rain')\n",
    "ax.set_ylim([0,25]);ax.set_ylabel('runoff')\n",
    "ax.set_title(f'sum of squares = {sum_of_squared_differences(X[0,:], y, [m_best, c_best]):3.2f}')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0f39a8",
   "metadata": {},
   "source": [
    "### 5.3 Model 2A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6efea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the training set (three data points)\n",
    "soil_condition=X[1,:3]\n",
    "runoff=y[:3]\n",
    "\n",
    "# find the best-fitting linear model\n",
    "    # define the loss function\n",
    "def sum_of_squared_differences(feature, runoff, pars):\n",
    "    m,c=pars\n",
    "    return np.sum([(runoff_i-m*feature_i-c)**2 for runoff_i,feature_i in zip(runoff,feature)])\n",
    "    # training\n",
    "m_best, c_best=minimize(partial(sum_of_squared_differences, soil_condition, runoff), [1,1]).x\n",
    "\n",
    "# show the result\n",
    "f,ax=plt.subplots(1,1,figsize=(6,3))\n",
    "ax.plot(X[1,:], y, 'o', ms=5, color=[1., 0.5, 0.5], label='all')\n",
    "ax.plot(X[1,:3], y[:3], 'o', ms=3, color=[0.2, 0.2, 0.8], label='train')\n",
    "ax.plot([0,7], [c_best,m_best*7+c_best], ':', color=[0.2, 0.2, 0.8], label=f'y={m_best:3.2f}x+{c_best:3.2f}')\n",
    "ax.set_xlim([0,7]);ax.set_xlabel('soil condition')\n",
    "ax.set_ylim([0,25]);ax.set_ylabel('runoff')\n",
    "ax.set_title(f'sum of squares = {sum_of_squared_differences(X[1,:], y, [m_best, c_best]):3.2f}')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d057e",
   "metadata": {},
   "source": [
    "### 5.4 Model 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed127a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the training set (all data points)\n",
    "soil_condition=X[1,:]\n",
    "runoff=y\n",
    "\n",
    "# find the best-fitting linear model\n",
    "    # use loss function defined above\n",
    "    # training\n",
    "from scipy.optimize import minimize     # find parameters that minimize the input\n",
    "from functools import partial           # locks the first N inputs to the function to particular values\n",
    "m_best, c_best=minimize(partial(sum_of_squared_differences, soil_condition, runoff), [1,1]).x\n",
    "\n",
    "# show the result\n",
    "f,ax=plt.subplots(1,1,figsize=(6,3))\n",
    "ax.plot(X[1,:], y, 'o', ms=5, color=[1., 0.5, 0.5], label='all')\n",
    "ax.plot(X[1,:], y[:], 'o', ms=3, color=[0.2, 0.2, 0.8], label='train')\n",
    "ax.plot([0,7], [c_best,m_best*7+c_best], ':', color=[1, 0.5, 0.5], label=f'y={m_best:3.2f}x+{c_best:3.2f}')\n",
    "ax.set_xlim([0,7]);ax.set_xlabel('soil_condition')\n",
    "ax.set_ylim([0,25]);ax.set_ylabel('runoff')\n",
    "ax.set_title(f'sum of squares = {sum_of_squared_differences(X[1,:], y, [m_best, c_best]):3.2f}')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc4331",
   "metadata": {},
   "source": [
    "### 5.5 Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4fa56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create feature matrix and label vector for training set\n",
    "X_train = X[:,:3].T\n",
    "y_train = y[:3]\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate an out-of-sample point for prediction\n",
    "X_test = np.array([[8.34, 5.91]])  # Test point with two features\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot the data points and the linear regression plane\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], y_train, color='blue', label='Training data')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], y_pred, color='red', label='Test prediction')\n",
    "\n",
    "# Create a meshgrid for plotting the regression plane\n",
    "x_grid, y_grid = np.meshgrid(np.linspace(0, 9, 50), np.linspace(0, 6, 50))\n",
    "z_grid = model.predict(np.c_[x_grid.ravel(), y_grid.ravel()]).reshape(x_grid.shape)\n",
    "ax.plot_surface(x_grid, y_grid, z_grid, alpha=0.5, cmap='viridis')#, label='Linear regression plane')\n",
    "\n",
    "ax.set_xlabel('rainfall')\n",
    "ax.set_ylabel('soil condition')\n",
    "ax.set_zlabel('runoff')\n",
    "ax.set_title('Simple Linear Regression with Two Features')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the prediction result\n",
    "print(f\"Predicted value for X_test: {y_pred[0]:3.2f}. Sum of squares = {(y_pred[0]-y[-1])**2:3.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2716b9",
   "metadata": {},
   "source": [
    "## 6. Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e2aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794883c",
   "metadata": {},
   "source": [
    "### 6.1 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460b39b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Consider the dataset below, describing seven bridges. For each, its load capacity, construction material and age is recorded.\n",
    "# An assessment of the bridge's safety has also been included, as a binary variable.\n",
    "data = [\n",
    "    {\"load_capacity\": 30, \"material_type\": \"Concrete\", \"age\": 5, \"safe\": True},\n",
    "    {\"load_capacity\": 60, \"material_type\": \"Steel\", \"age\": 15, \"safe\": True},\n",
    "    {\"load_capacity\": 70, \"material_type\": \"Concrete\", \"age\": 25, \"safe\": False},\n",
    "    {\"load_capacity\": 70, \"material_type\": \"Steel\", \"age\": 35, \"safe\": False},\n",
    "    {\"load_capacity\": 50, \"material_type\": \"Steel\", \"age\": 8, \"safe\": True},\n",
    "    {\"load_capacity\": 50, \"material_type\": \"Concrete\", \"age\": 10, \"safe\": False},\n",
    "    {\"load_capacity\": 35, \"material_type\": \"Steel\", \"age\": 3, \"safe\": True}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# We wish to develop a model that can predict whether a bridge is safe based on its load capacity, material and age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23814b33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For this problem, we will use a decision tree that splits the dataset into subsets.\n",
    "# Run this cell to see how a dataframe is split into two subsets, according to a feature and value\n",
    "feature='load_capacity'\n",
    "split=50\n",
    "print(df.loc[df[feature]<=split])\n",
    "print(df.loc[df[feature]>split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0a4b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from encn404 import decision_tree\n",
    "# run this cell below, and then use the controls below to complete the tasks\n",
    "decision_tree()\n",
    "\n",
    "# TASK 1\n",
    "# move the top slider to divide the dataset, trying both features\n",
    "# try to separate the safe and unsafe bridges as much as possible\n",
    "# when you are satisfied with the split of data, check the box to lock the root node\n",
    "\n",
    "# TASK 2\n",
    "# repeat the exercise for the lefthand and righthand sliders below\n",
    "# further separate and subdivide the data, trying to distinguish the two binary classes\n",
    "# can you construct a decision tree that classifies the two bridge types based on their features?\n",
    "\n",
    "# Consider the original dataframe given in the cells above. Which part is the feature matrix X, and\n",
    "# which is the label vector y?\n",
    "# What are the parameters of this model? What are the hyperparameters?\n",
    "\n",
    "# TASK 3\n",
    "# Suppose you are given a new bridge: load_capacity of 45, steel, and 10 years old. What would your model predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502a600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here is an AI written demo of sklearn's Decision Tree\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset (you can replace this with your own dataset)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929d69b",
   "metadata": {},
   "source": [
    "### 6.2 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013d474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neural network prediction of runoff, based on current and previous rainfall\n",
    "# note, all numbers have been normalized for convenience\n",
    "\n",
    "# feature matrix\n",
    "# columns: current rainfall, previous hour rainfall, previous runoff\n",
    "X = np.array([[0,0,1],        # rainfall has stopped, some runoff remains\n",
    "            [0,1,1],          # **describe**\n",
    "            [1,1,0],          # **describe**\n",
    "            [1,1,1]])         # **describe**\n",
    "\n",
    "# label vector: amount of current runoff\n",
    "y = np.array([[0.1],\n",
    "              [0.6],\n",
    "              [0.4],\n",
    "              [1.0]])\n",
    "\n",
    "# prediction: what will be the runoff for these features?\n",
    "predict=np.array([[0.5, 0.3, 0.8],])\n",
    "\n",
    "# run the cell below and then complete the tasks\n",
    "from encn404 import neural_network\n",
    "neural_network(X,y,predict)\n",
    "\n",
    "# TASK 0: model setup\n",
    "# consider the partial description of the feature matrix above, and complete the remaining rows\n",
    "# turn on the labels and read the descriptions of the model parts\n",
    "# each connector is an activation function - its color and thickness denote its weight (red=positive, blue=negative)\n",
    "\n",
    "# TASK 1: model training\n",
    "# (turn on labels if you are not sure what you are looking at)\n",
    "# how does the number of circles in the input layer reflect the input data?\n",
    "# is the number of circles in the hidden layer a parameter or a hyperparameter?\n",
    "\n",
    "# the model is initalized with random weights - use the slider to start training it\n",
    "# how does total error change as the number of training steps is increased?\n",
    "# how do the weights change during training?\n",
    "\n",
    "# use the slider to visualize a datapoint (a row of the feature matrix)\n",
    "# how does training improve the model prediction and the true label?\n",
    "\n",
    "# TASK 2: model prediction\n",
    "# which feature has the greatest total weight of connectors leaving it?\n",
    "# turn the prediction - does the answer make sense? Try a different prediction.\n",
    "\n",
    "# EXTRA:\n",
    "# make some changes to input features or labels and see how things change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea80a8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data with two predictors (X1 and X2)\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X1 = np.random.rand(n_samples, 1)\n",
    "X2 = np.random.rand(n_samples, 1)\n",
    "y = X1**3 + 2 * X2**2 + 3 * X1 * X2 + np.random.randn(n_samples, 1) * 2\n",
    "\n",
    "# Combine X1 and X2 into a single feature matrix\n",
    "X = np.hstack((X1, X2))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TASK\n",
    "# play with the hidden_layer_sizes, e.g., (10,10,10), max_iter (e.g., 10000), and other parameters\n",
    "# to see if you can improve the model fit.\n",
    "# Create an MLPRegressor model\n",
    "model = MLPRegressor(hidden_layer_sizes=(10, 5), activation='relu', solver='adam', max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Visualize the predictions (you can modify this part as needed)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_train, model.predict(X_train), color='r', alpha=0.7, label='in-sample')\n",
    "plt.scatter(y_test, y_pred, color='b', alpha=0.7, label='out-of-sample')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='r', linestyle='--')\n",
    "plt.xlabel(\"True values\")\n",
    "plt.ylabel(\"Predicted values\")\n",
    "plt.title(\"Cubic Function Regression with Neural Network\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f50c2",
   "metadata": {},
   "source": [
    "## 7. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e718e85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6302f79a",
   "metadata": {},
   "source": [
    "### 7.1 Classification models\n",
    "\n",
    "Consider a classification problem where we are trying to predict whether a site will be contaminated based on environmental indicators.\n",
    "\n",
    "'contamination'= 1 denotes a contaminated site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52351993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import and summarize the environmental data (last column is target/label)\n",
    "df=pd.read_csv('enviro_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bef604",
   "metadata": {},
   "source": [
    "We'll use a RandomForest model to fit these data and try solve the classification problem.\n",
    "- A random forest comprises lots of decision trees.\n",
    "- The number of trees in the forest is a hyperparameter.\n",
    "- The model output is a number between 0 and 1.\n",
    "- We convert the output to a binary prediction by comparing against a threshold value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12163b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this cell and answer the questions below\n",
    "from encn404 import roc\n",
    "roc()\n",
    "\n",
    "# TASK 1\n",
    "# for the default settings, referring to the confusion matrix:\n",
    "# - how many contaminated sites are in the dataset?\n",
    "# - how many contaminated sites are predicted by the model?\n",
    "# - what is the \"rate\" of correct model predictions?\n",
    "\n",
    "# TASK 2\n",
    "# change the slider so the threshold to predict contamination is lower\n",
    "# - which column gains more counts?\n",
    "# - does the sum across a row change? why/why not?\n",
    "# - which square in the conf. matrix is for false positives?\n",
    "# - increase the threshold to minimize false positives - what is the tradeoff?\n",
    "\n",
    "# TASK 3\n",
    "# change the number of trees\n",
    "# - how does hyperparameter selection affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b63ce79",
   "metadata": {},
   "source": [
    "### 7.2 Regression models\n",
    "\n",
    "Consider a regression problem where we are trying to fit the structural integrity score, $SI$, of a general building using a multivariate linear regression to several of its features, $X_i$.\n",
    "\n",
    "$$ SI = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\epsilon $$\n",
    "\n",
    "where the coefficients $\\beta_i$ are determined during model training and $\\epsilon$ is the remaining error (called the residual).\n",
    "\n",
    "The exercise below consider six possible features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c07d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this cell and answer the questions below\n",
    "from encn404 import regression_performance\n",
    "regression_performance()\n",
    "\n",
    "# TASK 1\n",
    "# inspect the output (without adjusting any of the checkboxes):\n",
    "# - what are six ways to evaluate how good a regression model is?\n",
    "# - does each measure give the same information about model performance?\n",
    "\n",
    "# TASK 2\n",
    "# use checkboxes to turn off just one feature\n",
    "# - does error increase or decrease when features are dropped?\n",
    "# - does each feature, when dropped, cause the same error change?\n",
    "# - could you use this behaviour to decide which features are most important?\n",
    "# - what appears to be the most important feature?\n",
    "\n",
    "# TASK 3\n",
    "# use checkboxes to turn off two features\n",
    "# - is the change in error additive? e.g., is the change in R-squared from \n",
    "# dropping two features the sum of the changes when those features are dropped \n",
    "# individually?\n",
    "\n",
    "# TASK 4\n",
    "# turn off checkboxes until only one feature remains\n",
    "# - what is the single most important feature in this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64835b00",
   "metadata": {},
   "source": [
    "## 8. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e726d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810b044",
   "metadata": {},
   "source": [
    "In this exercise, we'll look at some eye tracking data of the kind that might be analysed in a VR research study. In this case, the dataset records whether a subject's eyes are open or closed (the binary target). The features for prediction are continuous recordings from a set of 14 electrodes placed on the subject's head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884387de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import and summarize the environmental data (last column is target/label)\n",
    "df=pd.read_csv('eye_movement.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9d29d",
   "metadata": {},
   "source": [
    "The exercise below considers issues of **train-test split, hyperparameter tuning, and overfitting**, as they apply to this classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dce80cc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a28ebfa3c754dcab7525f6afb3124b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=70, continuous_update=False, description='Training Data Size (%)…"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell and answer the questions below\n",
    "from encn404 import cross_validation\n",
    "cross_validation()\n",
    "\n",
    "# TASK 1\n",
    "# inspect the output (without adjusting anything):\n",
    "# - what is the black line, and what does it mean when it switches back and forth?\n",
    "# - what are the blue and red lines?\n",
    "# - what do the two errors in the legend refer to?\n",
    "\n",
    "# TASK 2 - train-test split\n",
    "# slide the Training Data slider back and forth\n",
    "# - what is changing in terms of model input and predictions?\n",
    "# - is the error changing?\n",
    "\n",
    "# TASK 3 - hyperparameter tuning\n",
    "# slide the Max Depth slider so that trees can have more and more branches\n",
    "# - how does the fit between the blue and black line change?\n",
    "# - how does the error improve?\n",
    "\n",
    "# TASK 4 - overfitting\n",
    "# slide Max Depth to its largest value\n",
    "# - is this model good at fitting the data? does it have a low error?\n",
    "# - is this model any good at making predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c5b31",
   "metadata": {},
   "source": [
    "# Advanced Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344fe94",
   "metadata": {},
   "source": [
    "## 9. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fcd1eb",
   "metadata": {},
   "source": [
    "In this exercise, we'll look at what it means to call Neural Networks \"Universal Function Approximators\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6358f49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ab21f1d5eb4db9915e98e20186cb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(IntSlider(value=1, continuous_update=False, description='Hidden layers', max=8, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this cell and answer the questions below\n",
    "from encn404 import function_approximation\n",
    "function_approximation()\n",
    "\n",
    "# TASK 1\n",
    "# inspect the output (without adjusting anything):\n",
    "# - what are the orange and blue lines?\n",
    "# - what is the function that is being approximated?\n",
    "# - how many layers in this network? how many neurons per layer?\n",
    "\n",
    "# TASK 2 - function complexity\n",
    "# increase function complexity by adding a second term\n",
    "# - how does the function output change? will the simple neural network fit it?\n",
    "# - can you get a good fit by adding extra neurons only?\n",
    "# - set neurons to 4. Can you get a good fit by adding extra layers only?\n",
    "\n",
    "# TASK 3 - harder\n",
    "# increase to the hardest function level (three terms)\n",
    "# - can you get a good fit to the data?\n",
    "# - what does it mean for a neural network to be a function approximator?\n",
    "# - what does the 'universal' part mean?\n",
    "# - generally, how should a neural network change as function complexity increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc34d2",
   "metadata": {},
   "source": [
    "## 10. Physics-informed Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c02758",
   "metadata": {},
   "source": [
    "Open this [Binder notebook](https://mybinder.org/v2/gh/ddempsey/harmonic-oscillator-pinn/HEAD?urlpath=%2Fdoc%2Ftree%2FHarmonic+oscillator+PINN.ipynb) for an example of Physics-informed Machine Learning, adapted from Ben Moseley's classic example.\n",
    "\n",
    "<img src=\"img/pinn.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    " Answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4071a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1\n",
    "# Look at the class FCN definition.\n",
    "# - What keywords do you recognise and what might they correspond to?\n",
    "\n",
    "# TASK 2\n",
    "# Looking at the extent of the training data compared to the time domain\n",
    "# - Which time range of the solution should be classified as interpolation and which part as extrapolation?\n",
    "\n",
    "# TASK 3\n",
    "# Run the Normal Neural Network cell. \n",
    "# - Which line calculates the loss and how is it calculated? \n",
    "# - How does the NN solution improve with more epochs? \n",
    "# - In which time range does the solution perform well and where does it do worse? \n",
    "# - Why do you think this happens?\n",
    "\n",
    "# TASK 4\n",
    "# Run the PINN cell. \n",
    "# - Which line calculates the physics contribution to loss? \n",
    "# - Does it replace or complement the training data loss? \n",
    "# - How is performance improved even without new training data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f5245",
   "metadata": {},
   "source": [
    "## 11. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95815903",
   "metadata": {},
   "source": [
    "Grid World is a (dumb) game where the goal is to cross a desert and reach an oasis without dying. We will use it to learn the basics of reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell and answer the questions below\n",
    "from encn404 import GridWorldMDP\n",
    "GridWorldMDP()\n",
    "\n",
    "# TASK 1\n",
    "# Your current location is the green dot, and this can be moved by clicking the Step button.\n",
    "# - Take a few steps. What does the arrow tell us?\n",
    "# - What is the \"state\" for this problem?\n",
    "# - What is the \"state space\"?\n",
    "# - What is the \"action\"?\n",
    "# - What is the \"action space\"?\n",
    "\n",
    "# TASK 2\n",
    "# Moving about the desert costs energy. Reaching the oasis means you don't die.\n",
    "# - Take a step. How does the reward change?\n",
    "# - Keep taking steps until you reach the oasis. What is the total reward?\n",
    "\n",
    "# TASK 3\n",
    "# Color shows the \"value\" of each state, with red being good and blue bad.\n",
    "# - How does the value change as you approach the oasis? Why?\n",
    "# - How does value change on the non-oasis side of the black mountains? Why?\n",
    "# - How does value influence the proposed action?\n",
    "\n",
    "# TASK 4\n",
    "# The policy is our plan of action. \n",
    "# - Set the policy to \"random\" and travel to the oasis. Is this a smart?\n",
    "# - Set the policy to \"manual\" and travel to the oasis. How are you choosing direction?\n",
    "# - Set the policy to \"greedy\" and travel to the oasis. Quantitatively, what makes this better than \"random\"?\n",
    "\n",
    "# TASK 5\n",
    "# We can change the \"greedy\" policy.\n",
    "# - Slide gamma to zero. How do cell values change?\n",
    "# - Increase gamma up by 1 unit. How do cell values change?\n",
    "# - Keep increasing gamma. Do the directions change?  \n",
    "# - Does the policy dictate the state, the action or the reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead1d5db",
   "metadata": {},
   "source": [
    "One area that reinforcement learning has been used is to develop improved traffic light phasing policies. We'll consider an example of this below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dca6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell and answer the questions below\n",
    "from encn404 import TrafficSignalMDP\n",
    "TrafficSignalMDP()\n",
    "\n",
    "# TASK 0 \n",
    "# - The intersection below is grey. \n",
    "# - Traffic can travel either north-south or east-west.\n",
    "# - Traffic crosses the intersection at 2 veh/s on green signal. \n",
    "# - Traffic stops on yellow and red signals (so this is not Christchurch).\n",
    "# - Traffic arrives at Poisson rate Lambda (you can change this).\n",
    "# - NS and EW signals are green for a specified time (you can change this).\n",
    "# - (negative) reward builds up for vehicle wait time.\n",
    "# Take some steps. Take 20 steps. Watch the lights change and queues build up.\n",
    "\n",
    "# TASK 1\n",
    "# - Set NS traffic rate to maximum. Leave signals as default.\n",
    "# - After 120 seconds:\n",
    "#   o How are the queues? Which drivers are happy?\n",
    "#   o Write the cumulative reward here: \n",
    "# - Reset the model and double the length of the NS signal (20 s).\n",
    "# - After 120 seconds:\n",
    "#   o How are the queues? Which drivers are happy?\n",
    "#   o Write the cumulative reward here: \n",
    "# - Reset the model and halve both signal lengths (5 and 10 s).\n",
    "# - After 120 seconds:\n",
    "#   o How are the queues? Which drivers are happy? \n",
    "#   o Write the cumulative reward here:\n",
    "\n",
    "# TASK 2\n",
    "# - What is the \"state\" for this problem?\n",
    "# - What is the \"action\"?\n",
    "# - What is the \"policy\"?\n",
    "# - How does Reinforcement Learning direct us to improve the policy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ucnz)",
   "language": "python",
   "name": "ucnz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
